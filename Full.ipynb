{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f53f4a",
   "metadata": {},
   "source": [
    "### ÈåÑÈü≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1ac3d750-ee19-4b21-880f-098e482285a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from TTS.api import TTS\n",
    "from gliner import GLiNER\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80161f1-3c3e-4173-b39f-7e52505aa63c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 5018.31it/s]\n",
      "c:\\Users\\USER\\anaconda3\\envs\\graduateP\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\graduateP\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "c:\\Users\\USER\\anaconda3\\envs\\graduateP\\lib\\site-packages\\TTS\\utils\\io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "gn = GLiNER.from_pretrained(\"urchade/gliner_mediumv2.1\").to(device)\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\",  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b716d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"fairy_tales/1.txt\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#     content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be1c5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "THE LONG WAPPERS, AND THEIR TRICKS\n",
    "\n",
    "\n",
    "In his rambles in Belgium, the story-teller found no parts of any city\n",
    "in the land equal in interest to those of old Antwerp. If he sauntered\n",
    "down toward evening, into the narrow streets and through the stone\n",
    "gateway, blackened with age, under which the great Charles V. rode, the\n",
    "fairies and funny folks seemed almost as near to him as the figures in\n",
    "real history. Here, many a prince or princess made their ‚Äújoyous\n",
    "entry,‚Äù into the wonderful city of Brabo, the boy hero, who slew the\n",
    "cruel giant Antigonus and cut off his cruel hands.\n",
    "\n",
    "Here, the story-teller noticed a great many images of the Virgin Mary;\n",
    "whereas, in the newer parts of the city, there were few or none. They\n",
    "were usually set in the house corners, where two streets came together.\n",
    "Inquiring into the reason of this, he discovered a new kind of Belgian\n",
    "fairy, the Wapper, famous for his long legs and funny tricks. Here were\n",
    "fairies on stilts.\n",
    "\n",
    "But after five or ten minutes, when the supposed infant had drained\n",
    "both breasts, the woman thought of her own little one, in the cradle at\n",
    "home, and wondered whether her darling would have to go hungry.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7d15ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./Model/mistral-7b-instruct-v0.1.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  3917.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 32000\n",
      "llama_new_context_with_model: n_ctx_per_seq = 32000\n",
      "llama_new_context_with_model: n_batch       = 64\n",
      "llama_new_context_with_model: n_ubatch      = 64\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (32000) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4000.00 MiB, K (f16): 2000.00 MiB, V (f16): 2000.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   261.81 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '2', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "c:\\Users\\USER\\anaconda3\\envs\\graduateP\\lib\\site-packages\\llama_cpp\\llama.py:1237: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê®°ÂûãËºâÂÖ•ÊàêÂäüÔºÅ\n",
      "ÈñãÂßãÂàÜÊûêÊñáÊú¨...\n",
      "\n",
      "ÊâæÂà∞ 0 ÊÆµÂ∞çË©±:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   37302.05 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   560 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   318 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  121642.75 ms /   878 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ê®°ÂûãÂéüÂßãËº∏Âá∫:\n",
      "[\n",
      "{\"speaker\": \"The story-teller\", \"content\": \"In his rambles in Belgium, the story-teller found no parts of any city in the land equal in interest to those of old Antwerp. If he sauntered down toward evening, into the narrow streets and through the stone gateway, blackened with age, under which the great Charles V. rode, the fairies and funny folks seemed almost as near to him as the figures in real history. Here, many a prince or princess made their ‚Äòjoyous entry,‚Äô into the wonderful city of Brabo, the boy hero, who slew the cruel giant Antigonus and cut off his cruel hands.\"},\n",
      "{\"speaker\": \"The story-teller\", \"content\": \"Here, the story-teller noticed a great many images of the Virgin Mary; whereas, in the newer parts of the city, there were few or none. They were usually set in the house corners, where two streets came together. Inquiring into the reason of this, he discovered a new kind of Belgian fairy, the Wapper, famous for his long legs and funny tricks. Here were fairies on stilts.\"},\n",
      "{\"speaker\": \"The woman\", \"content\": \"But after five or ten minutes, when the supposed infant had drained both breasts, the woman thought of her own little one, in the cradle at home, and wondered whether her darling would have to go hungry.\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "result = llm(content)\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df['emotion'] = None\n",
    "df['char'] = None\n",
    "\n",
    "labels = [\"Male\", \"Female\", \"Other\"]\n",
    "\n",
    "# Áç≤ÂèñÊÉÖÁ∑íÂíåËßíËâ≤Ë∫´ÂàÜ\n",
    "for c in range(df.shape[0]):\n",
    "    sentence_with_name = f\"{df['speaker'][c]} : {df['content'][c]}\"\n",
    "    sentence_only = df['content'][c]\n",
    "    \n",
    "    classify_result = classifier(sentence_only)\n",
    "    gn_result = gn.predict_entities(sentence_with_name, labels, threshold=0.2)# thresholdÈÅéÈ´òÁÑ°Ê≥ïÂà§Êñ∑\n",
    "    \n",
    "    df.loc[c, 'emotion'] = classify_result[0]['label'] \n",
    "    df.loc[c, 'char'] = gn_result[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980326ff",
   "metadata": {},
   "source": [
    "## Image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccdb59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as Images/output_image_0.png\n",
      "Image saved as Images/output_image_1.png\n",
      "Image saved as Images/output_image_2.png\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/ZB-Tech/Text-to-Image\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.content\n",
    "\n",
    "for index, content in df['content'].items():\n",
    "\n",
    "    image_bytes = query({\"inputs\": content})\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    # display(image)\n",
    "\n",
    "    filename = f\"Images/output_image_{index}.png\"  # ÁîüÊàêÊñá‰ª∂Âêç\n",
    "    image.save(filename)  # ÂÑ≤Â≠òÁÇ∫ PNG Ê†ºÂºè\n",
    "    print(f\"Image saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "579ea641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['In his rambles in Belgium, the story-teller found no parts of any city in the land equal in interest to those of old Antwerp.', 'If he sauntered down toward evening, into the narrow streets and through the stone gateway, blackened with age, under which the great Charles V. rode, the fairies and funny folks seemed almost as near to him as the figures in real history.', 'Here, many a prince or princess made their ‚Äòjoyous entry,‚Äô into the wonderful city of Brabo, the boy hero, who slew the cruel giant Antigonus and cut off his cruel hands.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 20.149902820587158\n",
      " > Real-time factor: 0.5339900548934038\n",
      " > Text splitted to sentences.\n",
      "['Here, the story-teller noticed a great many images of the Virgin Mary; whereas, in the newer parts of the city, there were few or none.', 'They were usually set in the house corners, where two streets came together.', 'Inquiring into the reason of this, he discovered a new kind of Belgian fairy, the Wapper, famous for his long legs and funny tricks.', 'Here were fairies on stilts.']\n",
      " > Processing time: 11.909830570220947\n",
      " > Real-time factor: 0.4690031790657716\n",
      " > Text splitted to sentences.\n",
      "['But after five or ten minutes, when the supposed infant had drained both breasts, the woman thought of her own little one, in the cradle at home, and wondered whether her darling would have to go hungry.']\n",
      " > Processing time: 5.883904218673706\n",
      " > Real-time factor: 0.47141186566825777\n"
     ]
    }
   ],
   "source": [
    "speaker_mapping = {\n",
    "    'Female': \"Voice/03-01-01-01-01-01-06.wav\",\n",
    "    'Male': \"Voice/03-01-01-01-01-01-05.wav\",\n",
    "    'Other': \"Voice/03-01-01-01-01-01-11.wav\"\n",
    "}\n",
    "default_speaker = \"Voice/03-01-01-01-01-01-11.wav\"\n",
    "\n",
    "# ÊñáÂ≠óÂà∞Ë™ûÈü≥\n",
    "for d in range(df.shape[0]):\n",
    "    speaker_wav = speaker_mapping.get(df['char'][d], default_speaker)\n",
    "    \n",
    "    tts.tts_to_file(\n",
    "        text=df['content'][d],\n",
    "        # speaker_wav=speaker_wav,\n",
    "        speaker_wav='Voice/03-01-01-01-01-01-06.wav',\n",
    "        language=\"en\",\n",
    "        emotion=df['emotion'][d],\n",
    "        file_path=f\"Results/output{d}.wav\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce5278-de7f-49c9-a739-bd76c0561502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='Results/combined_audio.wav'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wav_files = [f\"Results/output{i}.wav\" for i in range(df.shape[0])]\n",
    "\n",
    "# combined_audio = AudioSegment.from_wav(wav_files[0])\n",
    "\n",
    "# # ÈÄêÊ≠•Â∞áÂâ©‰∏ãÁöÑÈü≥È†ªÊñá‰ª∂Âêà‰ΩµÂà∞Á¨¨‰∏ÄÂÄãÈü≥È†ªÊñá‰ª∂‰∏≠\n",
    "# for wav_file in wav_files[1:]:\n",
    "#     next_audio = AudioSegment.from_wav(wav_file)\n",
    "#     combined_audio += next_audio\n",
    "\n",
    "# combined_audio.export(\"Results/combined_audio.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f777f6",
   "metadata": {},
   "source": [
    "## Movie and Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "60a7b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import ImageClip, AudioFileClip, concatenate_videoclips, CompositeAudioClip, concatenate_audioclips\n",
    "\n",
    "output_video = \"Video/final_video2.mp4\"\n",
    "\n",
    "background_music_path = \"Music/converted_musicgen_out.wav\"\n",
    "background_music = AudioFileClip(background_music_path).with_volume_scaled(0.5)\n",
    "\n",
    "video_clips = []\n",
    "\n",
    "for index in range(len(df)):\n",
    "    image_path = f\"Images/output_image_{index}.png\"\n",
    "    audio_path = f\"Results/output{index}.wav\"\n",
    "    \n",
    "    # Âä†ËºâÈü≥È†ª\n",
    "    audio = AudioFileClip(audio_path)\n",
    "    \n",
    "    # Âä†ËºâÂúñÂÉè‰∏¶Ë®≠ÁΩÆÊåÅÁ∫åÊôÇÈñìÁÇ∫Èü≥È†ªÈï∑Â∫¶\n",
    "    image = ImageClip(image_path, duration=audio.duration)\n",
    "    \n",
    "    # Â∞áÂúñÂÉèÂíåÈü≥È†ªÁµêÂêà\n",
    "    video = image.with_audio(audio)\n",
    "    video_clips.append(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7a3d3a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂΩ±ÁâáÁîüÊàêÊàêÂäüÔºöVideo/final_video2.mp4\n"
     ]
    }
   ],
   "source": [
    "# Âêà‰ΩµÊâÄÊúâÁâáÊÆµ\n",
    "final_video = concatenate_videoclips(video_clips)\n",
    "\n",
    "# background_music = background_music.audio_loop(final_video.duration)\n",
    "background_music.time_transform(lambda t: t % final_video.duration)\n",
    "\n",
    "final_audio = CompositeAudioClip([final_video.audio, background_music])\n",
    "\n",
    "# Ë®≠ÁΩÆÊ∑∑ÂêàÂæåÁöÑÈü≥È†ªÁÇ∫ÂΩ±ÁâáÁöÑÈü≥È†ªËªåÈÅì\n",
    "final_video = final_video.with_audio(final_audio)\n",
    "final_video.write_videofile(\n",
    "    output_video,\n",
    "    codec=\"libx264\",\n",
    "    audio_codec=\"aac\",\n",
    "    temp_audiofile=\"temp-audio.m4a\",\n",
    "    remove_temp=True,\n",
    "    fps=24,\n",
    "    threads=4,\n",
    "    logger=None\n",
    ")\n",
    "\n",
    "print(f\"ÂΩ±ÁâáÁîüÊàêÊàêÂäüÔºö{output_video}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graduateP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
