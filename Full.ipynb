{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8b0324-c3e4-4fe5-8526-d1a16e84f324",
   "metadata": {},
   "source": [
    "1. LLMÁç≤ÂèñÊñáÂ≠ó\n",
    "2. Âà§Êñ∑‰∫∫Áâ©„ÄÅË™™Ë©±Ë™ûÊ∞£\n",
    "3. TTSËΩâË™ûÈü≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac3d750-ee19-4b21-880f-098e482285a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from TTS.api import TTS\n",
    "from gliner import GLiNER\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80161f1-3c3e-4173-b39f-7e52505aa63c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b7e761b3ea4ee093b056be129f3d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 has been updated, clearing model cache...\n",
      " > Downloading model to /Users/wangqiqian/Library/Application Support/tts/tts_models--multilingual--multi-dataset--xtts_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1.87G/1.87G [08:00<00:00, 4.30MiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.87G/1.87G [08:01<00:00, 3.88MiB/s]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.37k/4.37k [00:00<00:00, 9.91kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 361k/361k [00:00<00:00, 718kiB/s]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32.0/32.0 [00:00<00:00, 25.4kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model's license - CPML\n",
      " > Check https://coqui.ai/cpml.txt for more info.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "gn = GLiNER.from_pretrained(\"urchade/gliner_mediumv2.1\").to(device)\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\",  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5b1438d-75bc-4d75-8e42-9472c443183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('fairy_tales/1.txt', 'r', encoding='utf-8') as file:  # ‰ΩøÁî® 'utf-8' Á¢∫‰øùÊîØÊè¥‰∏≠ÊñáÊàñÂÖ∂‰ªñÁâπÊÆäÂ≠óÂÖÉ\n",
    "#     content = file.read()\n",
    "\n",
    "content = \"\"\"\n",
    "\"I don't think we should go any further,\" John said nervously.\n",
    "\"Come on, don't be such a coward,\" Sarah replied with a smirk.\n",
    "The forest was dark, and the wind howled through the trees.\n",
    "\"But what if we get lost?\" John asked, his voice trembling.\n",
    "\"We'll be fine,\" Sarah reassured him. \"I've been here before.\"\n",
    "They continued walking, the sound of crunching leaves underfoot the only noise accompanying them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c2fc2-2a40-4a13-9413-c95b63ba3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(df.shape[0]):\n",
    "    \n",
    "    tts.tts_to_file(\n",
    "        text=df['content'][d],\n",
    "        # speaker = df['char'][d],\n",
    "        speaker_wav=\"Data/03-01-01-01-01-01-06.wav\" if df['char'][d],\n",
    "        language=\"en\",\n",
    "        emotion = df['emotion'][d],\n",
    "        file_path=f\"Results/output{d}.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "636a7a0b-2045-4456-9b31-41d3b9e04b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./Model/mistral-7b-instruct-v0.1.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3917.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 514\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: llama-2\n",
      "/Users/wangqiqian/Library/Python/3.10/lib/python/site-packages/llama_cpp/llama.py:1238: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê®°ÂûãËºâÂÖ•ÊàêÂäüÔºÅ\n",
      "ÈñãÂßãÂàÜÊûêÊñáÊú¨...\n",
      "\n",
      "ÊâæÂà∞ 10 ÊÆµÂ∞çË©±:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   31969.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   365 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   77440.88 ms /   601 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ê®°ÂûãÂéüÂßãËº∏Âá∫:\n",
      "[\n",
      "    {\n",
      "        \"speaker\": \"John\",\n",
      "        \"content\": \"I don't think we should go any further, John said nervously.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Sarah\",\n",
      "        \"content\": \"Come on, don't be such a coward, Sarah replied with a smirk.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"The wind\",\n",
      "        \"content\": \"The forest was dark, and the wind howled through the trees.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"John\",\n",
      "        \"content\": \"But what if we get lost? John asked, his voice trembling.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Sarah\",\n",
      "        \"content\": \"We'll be fine, Sarah reassured him. I've been here before.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"The forest\",\n",
      "        \"content\": \"They continued walking, the sound of crunching leaves underfoot the only noise accompanying them.\"\n",
      "    }\n",
      "]\n",
      " > Text splitted to sentences.\n",
      "[\"I don't think we should go any further, John said nervously.\"]\n",
      " > Processing time: 16.629746198654175\n",
      " > Real-time factor: 3.232877553959696\n",
      " > Text splitted to sentences.\n",
      "[\"Come on, don't be such a coward, Sarah replied with a smirk.\"]\n",
      " > Processing time: 13.233092784881592\n",
      " > Real-time factor: 2.7005561963815996\n",
      " > Text splitted to sentences.\n",
      "['The forest was dark, and the wind howled through the trees.']\n",
      " > Processing time: 11.077285051345825\n",
      " > Real-time factor: 2.7100805008673827\n",
      " > Text splitted to sentences.\n",
      "['But what if we get lost?', 'John asked, his voice trembling.']\n",
      " > Processing time: 13.805842876434326\n",
      " > Real-time factor: 2.5565946269935576\n",
      " > Text splitted to sentences.\n",
      "[\"We'll be fine, Sarah reassured him.\", \"I've been here before.\"]\n",
      " > Processing time: 11.491306066513062\n",
      " > Real-time factor: 2.5049756679711033\n",
      " > Text splitted to sentences.\n",
      "['They continued walking, the sound of crunching leaves underfoot the only noise accompanying them.']\n",
      " > Processing time: 27.80666494369507\n",
      " > Real-time factor: 3.0585888838318915\n"
     ]
    }
   ],
   "source": [
    "result = llm(content)\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df['emotion'] = None\n",
    "df['char'] = None\n",
    "\n",
    "labels = [\"Male\", \"Female\", \"Other\"]\n",
    "\n",
    "# Áç≤ÂèñÊÉÖÁ∑íÂíåËßíËâ≤Ë∫´ÂàÜ\n",
    "for c in range(df.shape[0]):\n",
    "    sentence_with_name = f\"{df['speaker'][c]} : {df['content'][c]}\"\n",
    "    sentence_only = df['content'][c]\n",
    "    \n",
    "    classify_result = classifier(sentence_only)\n",
    "    gn_result = gn.predict_entities(sentence_with_name, labels, threshold=0.5)\n",
    "    \n",
    "    df.loc[c, 'emotion'] = classify_result[0]['label'] \n",
    "    df.loc[c, 'char'] = gn_result[0]['label']\n",
    "\n",
    "speaker_mapping = {\n",
    "    'Female': \"Data/03-01-01-01-01-01-06.wav\",\n",
    "    'Male': \"Data/03-01-01-01-01-01-05.wav\"\n",
    "}\n",
    "default_speaker = \"Data/03-01-01-01-01-01-11.wav\"\n",
    "\n",
    "# ÊñáÂ≠óÂà∞Ë™ûÈü≥\n",
    "for d in range(df.shape[0]):\n",
    "    speaker_wav = speaker_mapping.get(df['char'][d], default_speaker)\n",
    "    \n",
    "    tts.tts_to_file(\n",
    "        text=df['content'][d],\n",
    "        speaker_wav=speaker_wav,\n",
    "        language=\"en\",\n",
    "        emotion=df['emotion'][d],\n",
    "        file_path=f\"Results/output{d}.wav\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72ce5278-de7f-49c9-a739-bd76c0561502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='Results/combined_audio.wav'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_files = [f\"Results/output{i}.wav\" for i in range(0, 5)]\n",
    "\n",
    "combined_audio = AudioSegment.from_wav(wav_files[0])\n",
    "\n",
    "# ÈÄêÊ≠•Â∞áÂâ©‰∏ãÁöÑÈü≥È†ªÊñá‰ª∂Âêà‰ΩµÂà∞Á¨¨‰∏ÄÂÄãÈü≥È†ªÊñá‰ª∂‰∏≠\n",
    "for wav_file in wav_files[1:]:\n",
    "    next_audio = AudioSegment.from_wav(wav_file)\n",
    "    combined_audio += next_audio\n",
    "\n",
    "# Â∞áÂêà‰ΩµÂæåÁöÑÈü≥È†ªÊñá‰ª∂‰øùÂ≠ò\n",
    "combined_audio.export(\"Results/combined_audio.wav\", format=\"wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
