{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下載llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\USER\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 4/4 [1:05:03<00:00, 975.79s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login('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')  \n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoisaline\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 8448 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50941f7d6d064e70b433051ba1a3c034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "local_path = \"D:/llama/Meta-Llama-3-8B-Instruct\"  # 這裡填入您clone下來的模型資料夾路徑\n",
    "\n",
    "# 直接從本地載入\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_path,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",  # 自動處理設備分配\n",
    "    low_cpu_mem_usage=True  # 降低記憶體使用\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_llama(text):\n",
    "    # Prepare prompt in English\n",
    "    prompt = f\"\"\"Analyze the following story and identify who speaks each line of dialogue:\n",
    "\n",
    "Story:\n",
    "{text}\n",
    "\n",
    "For each piece of dialogue, please identify:\n",
    "1. Who is speaking\n",
    "2. What they say\n",
    "3. The context of their speech\n",
    "\n",
    "Please format your response as:\n",
    "Speaker: [character name]\n",
    "Quote: [what they said]\n",
    "---\"\"\"\n",
    "    \n",
    "    # Use model for analysis\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=1024)\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    print(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 讀取文件內容\n",
    "# with open('./fairy_tales/70.txt', 'r', encoding='utf-8') as file:\n",
    "with open('test_story.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "def process_text(text):\n",
    "    cleaned_text = text.encode('utf-8').decode('unicode-escape')\n",
    "    # 1. 處理常見的轉義序列\n",
    "    replacements = {\n",
    "        \"\\\\'\": \"'\",    # 單引號\n",
    "        '\\\\' : \"\",\n",
    "        '\\\\\"': '\"',    # 雙引號\n",
    "        '\\\\n': ' ',    # 換行符\n",
    "        '\\\\t': ' ',    # 製表符\n",
    "        '\\\\r': ' '     # 回車符\n",
    "    }\n",
    "    \n",
    "    cleaned_text = text\n",
    "    for old, new in replacements.items():\n",
    "        cleaned_text = cleaned_text.replace(old, new)\n",
    "    \n",
    "    # 2. 去掉多餘的空格\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "def split_hybrid_advanced(text, max_length=4000, preserve_words=True):\n",
    "    \"\"\"\n",
    "    進階混合分割方法\n",
    "    \n",
    "    Parameters:\n",
    "    - text: 要分割的文本\n",
    "    - max_length: 每個塊的最大字符數\n",
    "    - preserve_words: 是否在分割時保持單詞完整性\n",
    "    \"\"\"\n",
    "    def split_preserve_words(text, max_length):\n",
    "        \"\"\"在保持單詞完整性的情況下分割文本\"\"\"\n",
    "        if len(text) <= max_length:\n",
    "            return [text]\n",
    "        \n",
    "        # 找到最接近 max_length 的單詞邊界\n",
    "        split_point = max_length\n",
    "        while split_point > 0 and text[split_point] not in ' \\n\\t':\n",
    "            split_point -= 1\n",
    "            \n",
    "        if split_point == 0:\n",
    "            # 如果找不到合適的分割點，就直接在 max_length 處分割\n",
    "            split_point = max_length\n",
    "            \n",
    "        return [text[:split_point], text[split_point:]]\n",
    "    \n",
    "    # 主要邏輯與之前相似，但使用 split_preserve_words 進行分割\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_length = len(para)\n",
    "        \n",
    "        if para_length > max_length:\n",
    "            if current_chunk:\n",
    "                chunks.append('\\n\\n'.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "                \n",
    "            # 使用保持單詞完整性的分割方法\n",
    "            para_pieces = split_preserve_words(para, max_length)\n",
    "            chunks.extend(para_pieces[:-1])\n",
    "            if para_pieces[-1]:\n",
    "                current_chunk = [para_pieces[-1]]\n",
    "                current_length = len(para_pieces[-1])\n",
    "        else:\n",
    "            if current_length + para_length + 2 > max_length:\n",
    "                chunks.append('\\n\\n'.join(current_chunk))\n",
    "                current_chunk = [para]\n",
    "                current_length = para_length\n",
    "            else:\n",
    "                current_chunk.append(para)\n",
    "                current_length += para_length + 2\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append('\\n\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = split_hybrid_advanced(text)\n",
    "story_list = []\n",
    "for i in story:\n",
    "    sentence = process_text(i)\n",
    "    story_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE MAGIC PAINTBRUSH Little Ming lived in a small village by the mountains. Every day she would sit by her window and dream of painting the world in beautiful colors, but her family was too poor to buy art supplies. One night, an old woman appeared in her dreams and said: \"Your kind heart deserves a gift.\" When Ming woke up, she found a paintbrush on her pillow. That morning, Ming drew a bird on paper, and to her amazement, it came to life and flew away! \"I can\\'t believe my eyes!\" she exclaimed. \"What a wonderful paintbrush!\" said her mother, watching Ming paint rice bowls that filled with real rice. The village chief heard about the magic paintbrush and demanded: \"Give me that brush, and I\\'ll make you rich!\" \"The brush is meant to help those in need,\" Ming replied firmly. When the chief tried to steal it, the brush painted a strong wind that blew him far away. From that day on, Ming used her gift to help the villagers, painting rain during droughts and warm clothes for the poor. \"Magic is best used with a pure heart,\" Ming would say to anyone who asked about her special paintbrush.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Analyze the following story and identify who speaks each line of dialogue:\n",
      "\n",
      "Story:\n",
      "THE MAGIC PAINTBRUSH Little Ming lived in a small village by the mountains. Every day she would sit by her window and dream of painting the world in beautiful colors, but her family was too poor to buy art supplies. One night, an old woman appeared in her dreams and said: \"Your kind heart deserves a gift.\" When Ming woke up, she found a paintbrush on her pillow. That morning, Ming drew a bird on paper, and to her amazement, it came to life and flew away! \"I can't believe my eyes!\" she exclaimed. \"What a wonderful paintbrush!\" said her mother, watching Ming paint rice bowls that filled with real rice. The village chief heard about the magic paintbrush and demanded: \"Give me that brush, and I'll make you rich!\" \"The brush is meant to help those in need,\" Ming replied firmly. When the chief tried to steal it, the brush painted a strong wind that blew him far away. From that day on, Ming used her gift to help the villagers, painting rain during droughts and warm clothes for the poor. \"Magic is best used with a pure heart,\" Ming would say to anyone who asked about her special paintbrush.\n",
      "\n",
      "For each piece of dialogue, please identify:\n",
      "1. Who is speaking\n",
      "2. What they say\n",
      "3. The context of their speech\n",
      "\n",
      "Please format your response as:\n",
      "Speaker: [character name]\n",
      "Quote: [what they said]\n",
      "--- Context: [context of their speech]\n",
      "\n",
      "For example:\n",
      "Speaker: Little Ming\n",
      "Quote: I can't believe my eyes!\n",
      "--- Context: After drawing a bird on paper, it came to life and flew away!\n",
      "\n",
      "Thank you for your help!<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "for para in story_list:\n",
    "    analyze_with_llama(para)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
